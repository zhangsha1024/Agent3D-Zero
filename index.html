<!DOCTYPE html>
<html>

<head lang="en">
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Agent3D-Zero</title>

    <!-- <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://dorverbin.github.io/refnerf/img/refnerf_titlecard.jpg">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://dorverbin.github.io/refnerf">
    <meta property="og:title" content="Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields">
    <meta property="og:description"
        content="Neural Radiance Fields (NeRF) is a popular view synthesis technique that represents a scene as a continuous volumetric function, parameterized by multilayer perceptrons that provide the volume density and view-dependent emitted radiance at each location. While NeRF-based techniques excel at representing fine geometric structures with smoothly varying view-dependent appearance, they often fail to accurately capture and reproduce the appearance of glossy surfaces. We address this limitation by introducing Ref-NeRF, which replaces NeRF's parameterization of view-dependent outgoing radiance with a representation of reflected radiance and structures this function using a collection of spatially-varying scene properties. We show that together with a regularizer on normal vectors, our model significantly improves the realism and accuracy of specular reflections. Furthermore, we show that our model's internal representation of outgoing radiance is interpretable and useful for scene editing.">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields">
    <meta name="twitter:description"
        content="Neural Radiance Fields (NeRF) is a popular view synthesis technique that represents a scene as a continuous volumetric function, parameterized by multilayer perceptrons that provide the volume density and view-dependent emitted radiance at each location. While NeRF-based techniques excel at representing fine geometric structures with smoothly varying view-dependent appearance, they often fail to accurately capture and reproduce the appearance of glossy surfaces. We address this limitation by introducing Ref-NeRF, which replaces NeRF's parameterization of view-dependent outgoing radiance with a representation of reflected radiance and structures this function using a collection of spatially-varying scene properties. We show that together with a regularizer on normal vectors, our model significantly improves the realism and accuracy of specular reflections. Furthermore, we show that our model's internal representation of outgoing radiance is interpretable and useful for scene editing.">
    <meta name="twitter:image" content="https://dorverbin.github.io/refnerf/img/refnerf_titlecard.jpg"> -->


    <!-- mirror: F0%9F%AA%9E&lt -->
    <!-- <link rel="icon"
        href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text y=%22.9em%22 font-size=%2290%22&gt;%E2%9C%A8&lt;/text&gt;&lt;/svg&gt;"> -->
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/font-awesome.min.css">
    <link rel="stylesheet" href="css/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/codemirror.min.js"></script>
    <script src="js/clipboard.min.js"></script>
    <script src="js/video_comparison.js"></script>
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="header" style="text-align: center; margin: auto;">
        <div class="row" id="title-row" style="max-width: 100%; margin: 0 auto; display: inline-block">
            <h2 class="col-md-12 text-center" id="title">
                <b>Agent3D-Zero</b> <br>
                <br>
                An Agent for 
                Zero-shot 3D Understanding  <br>
                <small>
                    Arxiv 2024
                </small>
            </h2>
        </div>
        <video id="materials" width="60%"  autoplay muted controls style="border: 2px solid #000;">
            <source src="video/output_4.mp4" type="video/mp4" />
        </video>
        <div class="row" id="author-row" style="margin:0 auto;">
            <div class="col-md-12 text-center" style="display: table; margin:0 auto">
                <table class="author-table" id="author-table">
                    <tr>
                        <td>
                            <a style="text-decoration:none" href="https://github.com/zhangsha1024/">
                                Sha Zhang
                            </a>
                            <br>The University of Science and Technology of China
                            <!-- <br>Shanghai AI Laboratory -->
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://dihuang.me/">
                                Di Huang
                            </a>
                            <br>The University of Sydney<br>Shanghai AI Laboratory
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://djiajunustc.github.io">
                                Jiajun Deng
                            </a>
                            <br>The University of Adelaide
                        </td>
                    </tr>
                    <tr>
                        <td>
                            <a style="text-decoration:none" href="https://scholar.google.com.tw/citations?user=TJ4ihdkAAAAJ&hl=en">
                                Shixiang Tang
                            </a>
                            <br>The University of Sydney
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://wlouyang.github.io/">
                                Wanli Ouyang
                            </a>
                            <br>Shanghai AI Laboratory
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://tonghe90.github.io/">
                                Tong He
                            </a>
                            <br>Shanghai AI Laboratory
                        </td>
                    </tr>
                    <tr>
                        <td></td>
                        <td>
                            <a style="text-decoration:none" href="http://staff.ustc.edu.cn/~yanyongz/">
                                Yanyong Zhang*
                            </a>
                            <br>The University of Science and Technology of China
                        </td>
                    </tr>
                </table>
            </div>
        </div>
    </div>
    <script>
        document.getElementById('author-row').style.maxWidth = document.getElementById("title-row").clientWidth + 'px';
    </script>
    

    <div class="container" id="main">
        <div class="row">
            <div class="col-sm-6 col-sm-offset-3 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="https://arxiv.org/abs/2403.11835">
                            <img src="img/paper_page.png" height="60px">
                            <h4><strong>Paper</strong></h4>
                        </a>
                    </li>
                    <!-- <li>
                        <a href="https://arxiv.org/abs/2310.08586">
                            <img src="img/Snipaste_2023-10-26_20-24-39.png" height="60px">
                            <h4><strong>PonderV2 Paper</strong></h4>
                        </a>
                    </li> -->
                    <!-- <li>
                            <a href="https://youtu.be/qrdRH9irAlk">
                            <img src="./img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li> -->
                    <!-- <li>
                            <a href="https://storage.googleapis.com/gresearch/refraw360/ref.zip" target="_blank">
                            <image src="img/database_icon.png" height="60px">
                                <h4><strong>Shiny Dataset</strong></h4>
                            </a>
                        </li> -->
                    <!-- <li>  
                            <a href="https://dorverbin.github.io/refnerf/data/shiny_blender_source.zip" target="_blank">
                            <image src="img/database_icon.png" height="60px">
                                <h4><strong>Shiny Dataset Source</strong></h4>
                            </a>
                        </li> -->
                    <!-- <li>
                            <a href="https://storage.googleapis.com/gresearch/refraw360/ref_real.zip" target="_blank">
                            <image src="img/real_database_icon.png" height="60px">
                                <h4><strong>Real Dataset</strong></h4>
                            </a>
                        </li>                             -->
                    <li>
                        <a href="https://github.com/zhangsha1024/Agent3d-zero-code" target="_blank">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                        </a>
                    </li>
                </ul>
            </div>
        </div>

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <div class="video-compare-container" id="materialsDiv">
                    <video class="video" id="materials" loop playsinline autoPlay muted src="video/project_page.mov" onplay="resizeAndPlay(this)"></video> -->
                    <!-- <video id="materials" width="40%"  autoplay muted controls>
                        <source src="video/project_page.mov" type="video/quicktime" />
                    </video> -->
                    <!-- <canvas height=0 class="videoMerge" id="materialsMerge"></canvas> -->
                <!-- </div>
			</div>
        </div> -->
        <!-- <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h3>
                        Reflection Direction Parameterization
                    </h3>
                    <div class="text-justify">
                        Previous approaches directly input the camera's view direction into the MLP to predict outgoing
                        radiance. We show that instead using the reflection of the view direction about the normal makes
                        the emittance function significantly easier to learn and interpolate, greatly improving our
                        results.

                        <br><br>

                    </div>
                    <div class="text-center">
                        <video id="refdir" width="40%" playsinline autoplay loop muted>
                            <source src="video/reflection_animation.mp4" type="video/mp4" />
                        </video>
                    </div>
                </div>
            </div> -->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract & Pipeline
                </h3>
                <p class="text-justify">
                    The ability to understand and reason the 3D real world is a crucial milestone towards artificial general intelligence. 
                    The current common practice is to finetune Large Language Models (LLMs) with 3D data and texts to enable 3D understanding. 
                    Despite their effectiveness, these approaches are inherently limited by the scale and diversity of the available 3D data.
                    Alternatively, in this work, we introduce Agent3D-Zero, an innovative 3D-aware agent framework addressing the 3D scene understanding in a zero-shot manner. 
                    The essence of our approach centers on reconceptualizing the challenge of 3D scene perception as a process of understanding and synthesizing insights from multiple images, inspired by how our human beings attempt to understand 3D scenes.
                    By consolidating this idea, we propose a novel way to make use of a Large Visual Language Model (VLM) via actively selecting and analyzing a series of viewpoints for 3D understanding.  
                    Specifically, given an input 3D scene, Agent3D-Zero first processes a bird's-eye view image with custom-designed visual prompts, then iteratively chooses the next viewpoints to observe and summarize the underlying knowledge.
                    A distinctive advantage of Agent3D-Zero is the introduction of novel visual prompts, which significantly unleash the VLMs' ability to identify the most informative viewpoints and thus facilitate observing 3D scenes. 
                    Extensive experiments demonstrate the effectiveness of the proposed framework in understanding diverse and previously unseen 3D environments.
                </p>
            </div>
        </div>

        <image src="img/final_overview.png" class="img-responsive" alt="overview" width="60%"
            style="max-height: 450px;margin:auto;">
            <br>

            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <p class="text-justify">
                        We initiate the process by overlaying grid lines and tick marks on the Bird's Eye View (BEV) images, 
                        constituting the prompt along with a scene type description. 
                        This prompt guides the Vision Language Model (VLM) to retrieve camera poses for images observing the 3D scene. 
                        The lower section demonstrates the versatility of Agent3D-Zero, showcasing its proficiency in addressing various 3D reasoning and perception tasks through strategic prompting and tool utilization.
                    </p>
                </div>
            </div>

            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h3>
                        Performance
                    </h3>
                    <div class="text-justify">
                        Agent3D-Zero achieves state-of-the-art performance on ScanQA task. Here is the ranks in ScanQA test.
                    </div>
                    <br>
                    <div class="text-center">
                        <img src="img/scanqa_test.jpg" width="100%">
                    </div>
                    <!-- <div class="text-center">
                        <img src="img/2beb9025cd119848353c6a2c6a3a6015.PNG" width="100%">
                    </div>
                    <br> -->
                    <!-- <div class="text-justify">
                        We call this <i>Integrated Directional Encoding</i>, and we show experimentally that it allows
                        sharing the emittance functions between points with different roughnesses. It also enables scene
                        editing after training. Theoretically, our encoding is stationary on the sphere, similar to the
                        Euclidean stationarity of NeRF's positional encoding. <br><br>
                    </div> -->
                </div>
            </div>

            <!-- <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h3>
                        Video
                    </h3>
                    <div class="text-center">
                        <div style="position:relative;padding-top:56.25%;">
                            <iframe src="https://youtube.com/embed/qrdRH9irAlk" allowfullscreen
                                style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                        </div>
                    </div>
                </div>
            </div> -->

            <!-- <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h3>
                        Reflection Direction Parameterization
                    </h3>
                    <div class="text-justify">
                        Previous approaches directly input the camera's view direction into the MLP to predict outgoing
                        radiance. We show that instead using the reflection of the view direction about the normal makes
                        the emittance function significantly easier to learn and interpolate, greatly improving our
                        results.

                        <br><br>

                    </div>
                    <div class="text-center">
                        <video id="refdir" width="40%" playsinline autoplay loop muted>
                            <source src="video/reflection_animation.mp4" type="video/mp4" />
                        </video>
                    </div>
                </div>
            </div> -->

            <!-- <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h3>
                        Integrated Directional Encoding
                    </h3>
                    <div class="text-justify">
                        We explicitly model object roughness using the expected values of a set of spherical harmonics
                        under a von Mises-Fisher distribution whose concentration parameter varies spatially:
                    </div>
                    <br>
                    <div class="text-center">
                        <img src="./img/ide.png" width="50%">
                    </div>
                    <br>
                    <div class="text-justify">
                        We call this <i>Integrated Directional Encoding</i>, and we show experimentally that it allows
                        sharing the emittance functions between points with different roughnesses. It also enables scene
                        editing after training. Theoretically, our encoding is stationary on the sphere, similar to the
                        Euclidean stationarity of NeRF's positional encoding. <br><br>
                    </div>
                    <div class="text-center">
                        <video id="ide" width="100%" playsinline autoplay controls loop muted>
                            <source src="video/ide_animation.mp4" type="video/mp4" />
                        </video>
                    </div>
                </div>
            </div> -->

            <!-- <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h3>
                        Additional Synthetic Results
                    </h3>
                    <div class="video-compare-container">
                        <video class="video" id="musclecar" loop playsinline autoPlay muted
                            src="video/musclecar_mipnerf_ours.mp4" onplay="resizeAndPlay(this)"></video>
                        <canvas height=0 class="videoMerge" id="musclecarMerge"></canvas>
                    </div>
                </div>
            </div> -->

            <!-- <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h3>
                        Results on Captured Scenes
                    </h3>
                    Our method also produces accurate renderings and surface normals from captured photographs:
                    <div class="video-compare-container" style="width: 100%">
                        <video class="video" id="toycar" loop playsinline autoPlay muted src="video/toycar.mp4"
                            onplay="resizeAndPlay(this)"></video>
                        <canvas height=0 class="videoMerge" id="toycarMerge"></canvas>
                    </div>
                </div>
            </div> -->

            <!-- <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h3>
                        Scene Editing
                    </h3>
                    <div class="text-justify">
                        We show that our structured representation of the directional MLP allows for scene editing after
                        training. Here we show that we can convincingly change material properties.
                        <br>
                        We can increase and decrease material roughness:
                    </div>

                    <div style="overflow: hidden;">
                        <video id="editing-materials" width="100%" playsinline autoplay loop muted
                            style="margin-top: -5%;">
                            <source src="video/materials_rougher_smoother.mp4" type="video/mp4" />
                        </video>
                    </div>

                    <div class="text-justify">
                        We can also control the amounts of specular and diffuse colors, or change the diffuse color
                        without affecting the specular reflections:
                    </div>

                    <table width="100%">
                        <tr>
                            <td align="left" valign="top" width="50%">
                                <video id="v2" width="100%" playsinline autoplay loop muted>
                                    <source src="video/car_color2.mp4" type="video/mp4" />
                                </video>
                            </td>
                            <td align="left" valign="top" width="50%">
                                <video id="v3" width="100%" playsinline autoplay loop muted>
                                    <source src="video/car_color3.mp4" type="video/mp4" />
                                </video>
                            </td>
                        </tr>
                    </table>

                </div>
            </div> -->


            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h3>
                        Citation
                    </h3>
                    <!-- <div class="form-group col-md-10 col-md-offset-1">
                        <textarea id="bibtex" class="form-control" readonly>
@inproceedings{huang2023ponder,
    title={Ponder: Point cloud pre-training via neural rendering},
    author={Huang, Di and Peng, Sida and He, Tong and Yang, Honghui and Zhou, Xiaowei and Ouyang, Wanli},
    booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
    pages={16089--16098},
    year={2023}
  }
</textarea>
                    </div> -->
                </div>
            </div>

            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <!-- <h3>
                        Acknowledgements
                    </h3> -->
                    <p class="text-justify">
                        <!-- We would like to thank Lior Yariv and Kai Zhang for helping us evaluate their methods, and
                        Ricardo Martin-Brualla for helpful comments on our text. DV is supported by the National Science
                        Foundation under Cooperative Agreement PHY-2019786 (an NSF AI Institute, <a
                            href="http://iaifi.org">http://iaifi.org</a>)
                        <br> -->
                        The website template was borrowed from  <a
                            href="https://dihuangdh.github.io/ponder/">Ponder</a>. Thanks to them!
                    </p>
                </div>
            </div>
    </div>


<!-- Default Statcounter code for Ponder
https://dihuangdh.github.io/ponder -->
<script type="text/javascript">
    var sc_project=12946926; 
    var sc_invisible=1; 
    var sc_security="d7ea4aa9"; 
    </script>
    <script type="text/javascript"
    src="https://www.statcounter.com/counter/counter.js"
    async></script>
    <noscript><div class="statcounter"><a title="Web Analytics"
    href="https://statcounter.com/" target="_blank"><img
    class="statcounter"
    src="https://c.statcounter.com/12946926/0/d7ea4aa9/1/"
    alt="Web Analytics"
    referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
    <!-- End of Statcounter Code -->

</body>

</html>
